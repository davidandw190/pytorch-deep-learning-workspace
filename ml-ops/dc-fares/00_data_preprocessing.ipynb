{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G8d8oAmUHr7o"
      ],
      "authorship_tag": "ABX9TyPbAEH4GcXAbhHh6IG9Ee2i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ML-Ops | DC Taxi Fares**\n",
        "\n",
        "## 00 - Data Preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "zbLzYmEoFNal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the DC taxi dataset"
      ],
      "metadata": {
        "id": "G8d8oAmUHr7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`gdown` is a Python utility for downloading files stored in Google Drive. The bash script in the following cell iterates through a collection of Google Drive identifiers that match files `taxi_2015.zip` through `taxi_2019.zip` stored in a shared Google Drive. This script uses these files instead of the original files from https://opendata.dc.gov/search?categories=transportation&q=taxi&type=document%20link since the originals cannot be easily downloaded using a bash script."
      ],
      "metadata": {
        "id": "CcZHfYwrH13-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install gdown\n",
        "for ID in '1yF2hYrVjAZ3VPFo1dDkN80wUV2eEq65O'\\\n",
        "          '1Z7ZVi79wKEbnc0FH3o0XKHGUS8MQOU6R'\\\n",
        "          '1I_uraLKNbGPe3IeE7FUa9gPfwBHjthu4'\\\n",
        "          '1MoY3THytktrxam6_hFSC8kW1DeHZ8_g1'\\\n",
        "          '1balxhT6Qh_WDp4wq4OsG40iwqFa86QgW'\n",
        "do\n",
        "\n",
        "  gdown --id $ID\n",
        "\n",
        "done"
      ],
      "metadata": {
        "id": "gNjWRlllIEM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unziping the dataset\n",
        "\n",
        "The script in the following cell unzips the downloaded dataset files to the `dctaxi` subdirectory in the current directory of the notebook.\n",
        "\n",
        "The `-o` flag used by the `unzip` command overwrites existing files in case we execute the next cell more than once.\n"
      ],
      "metadata": {
        "id": "5C798gCfIGlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "mkdir -p dctaxi\n",
        "\n",
        "for YEAR in '2015' \\\n",
        "            '2016' \\\n",
        "            '2017' \\\n",
        "            '2018' \\\n",
        "            '2019'\n",
        "do\n",
        "\n",
        "  unzip -o taxi_$YEAR.zip -d dctaxi\n",
        "\n",
        "done"
      ],
      "metadata": {
        "id": "S20gIuSAIeVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reporting on the disk space used by the dataset files\n",
        "\n",
        "The next cell reports on the disk usage (`du`) by the files from the DC taxi dataset. All of the files in the dataset have the `taxi_` prefix.\n",
        "\n",
        "Since the entire output of the `du` command lists the disk usage of all of the files, the `tail` command is used to limit the output to just the last line. We can remove the tail command we wish to report on the disk usage by the individual files in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "eL5AhfzoIiSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!du -cha --block-size=1MB dctaxi/taxi_*.txt | tail -n 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLPQcoJQJTdl",
        "outputId": "90ea5bbc-1360-42ea-bf33-a3617c1074a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11987\ttotal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scanning the dataset documentation\n",
        "\n",
        "The dataset includes a `README_DC_Taxicab_trip.txt` file with a brief documentation about the dataset contents. We will run the next cell and take a moment to review the documentation, focusing on the schema used by the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "cLDxCbHdJS2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat dctaxi/README_DC_Taxicab_trip.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KdZ1aITJwgy",
        "outputId": "52136365-0474-413d-eaee-269a94e12009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TITLE: Taxicab Trip Information\r\n",
            "\r\n",
            "ABSTRACT:  DC Taxi trips from January 2015 to June 2019.  The zip file contains zip files of pipe (|) delimeted text files for trips by month.  The record counts are:\r\n",
            "\r\n",
            "Month\t\t\tCount\r\n",
            "5/2015\t\t1,397,102 \r\n",
            "6/2015\t\t1,470,466 \r\n",
            "7/2015\t\t1,401,792 \r\n",
            "8/2015\t\t1,129,707 \r\n",
            "9/2015\t\t1,308,445 \r\n",
            "10/2015\t\t1,487,133 \r\n",
            "11/2015\t \t  993,502 \r\n",
            "12/2015\t\t1,081,726 \r\n",
            "1/2016\t \t  922,338 \r\n",
            "2/2016\t\t1,194,698 \r\n",
            "3/2016\t\t1,404,639 \r\n",
            "4/2016\t\t1,369,882 \r\n",
            "5/2016\t\t1,323,155 \r\n",
            "6/2016\t\t1,282,651 \r\n",
            "7/2016\t\t1,109,118 \r\n",
            "8/2016\t\t  949,650 \r\n",
            "9/2016\t\t1,203,388 \r\n",
            "10/2016\t\t1,027,036 \r\n",
            "11/2016\t\t1,011,673 \r\n",
            "12/2016\t \t  898,993 \r\n",
            "1/2017\t  \t  901,807 \r\n",
            "2/2017\t \t  949,578 \r\n",
            "3/2017\t\t1,237,621\r\n",
            "4/2017\t\t1,185,859\r\n",
            "5/2017\t\t1,188,944\r\n",
            "6/2017\t\t1,182,526\r\n",
            "7/2017\t\t  993,834\r\n",
            "8/2017     \t  813,513\r\n",
            "9/2017\t\t  902,795\r\n",
            "10/2017    \t1,056,864\r\n",
            "11/2017    \t  867,709\r\n",
            "12/2017    \t  730,679\r\n",
            "1/2018     \t  686,993\r\n",
            "2/2018     \t  750,363\r\n",
            "3/2018     \t  999,048\r\n",
            "4/2018     \t1,040,147\r\n",
            "5/2018     \t  984,593\r\n",
            "6/2018     \t  941,608\r\n",
            "7/2018     \t  810,292\r\n",
            "8/2018     \t  735,262\r\n",
            "9/2018    \t  819,233\r\n",
            "10/2018�� ��\t  900,574\r\n",
            "11/2018���� \t  753,670\r\n",
            "12/2018���� \t  670,970\r\n",
            "01/2019���� \t  589,475\r\n",
            "02/2019���� \t  667,546\r\n",
            "03/2019���� \t  902,382\r\n",
            "04/2019���� \t  853,950\r\n",
            "05/2019���� \t  865,374\r\n",
            "06/2019����\t  800,030\r\n",
            "\r\n",
            "\r\n",
            "DISCLAIMER:�This work is licensed under a Creative Commons Attribution 4.0 International License.\r\n",
            "\r\n",
            "CATEGORY: Public Service\t\r\n",
            "\r\n",
            "PROVIDER: Office of the Chief Technology Officer (OCTO)\r\n",
            "\r\n",
            "ORIGINATOR: Department of For-Hire Vehicles (DFHV)\r\n",
            "\r\n",
            "PROCESS DESCRIPTION: DFHV provided OCTO with a taxicab trip text file representing trips from January 2015 to June 2019.  OCTO processed the data to assign a block locations to pick up and drop off locations.  The blocks were assigned using the original pick up, drop off lat/long coordinates and searching for the block locations in the DC Master Address Repository (radius tolerance of 250 meters and less).  The pick and drop off times were also rounded to the nearest hour.\r\n",
            "\r\n",
            "In addition, the pick up and drop off locations were assigned to an airport using locator polygons for Reagan, BWI, and Dulles.  These polygons generally followed the visual borders of these airports.\r\n",
            "\r\n",
            "The block assignment metrics are as follows\r\n",
            " \r\n",
            "\t\t\t\t\tPick Up Location\t    Drop Off Location\t   \r\n",
            "Total Records (overall):\t\t53,173,692\t\t\t53,173,692\r\n",
            "Percent Geocoded to block:\t\t82%\t             \t\t 75%\t    \r\n",
            "\t\t\t \r\n",
            "For more information, go to http://dfhv.dc.gov\r\n",
            "\r\n",
            "TABLE STRUCTURE:\t \r\n",
            "\r\n",
            "COLUMN_NAME\t\tDATA_TYPE\t\tDESCRIPTION\r\n",
            "OBJECTID\t\t\tNUMBER(9)\t\tTable Unique Identifier\t\r\n",
            "TRIPTYPE\t\t\tVARCHAR2(255)\tType of Taxi Trip\r\n",
            "PROVIDERNAME\t\tVARCHAR2(255)\tTaxi Company that Provided \t\t\t\t\t\t\t\ttrip\t\r\n",
            "FAREAMOUNT\t\t\tVARCHAR2(255)\tMeter Fare Amount\r\n",
            "GRATUITYAMOUNT\t\tVARCHAR2(255)\tTip amount\r\n",
            "SURCHARGEAMOUNT\t\tVARCHAR2(255)\tSurcharge fee\r\n",
            "EXTRAFAREAMOUNT\t\tVARCHAR2(255)\tExtra fees\r\n",
            "TOLLAMOUNT\t\t\tVARCHAR2(255)\tToll amount\r\n",
            "TOTALAMOUNT\t\tVARCHAR2(255)\tTotal amount from Meter \t\t\t\t\t\t\t\tfare, tip, surcharge, \t\t\t\t\t\t\t\textras, and tolls. \r\n",
            "PAYMENTTYPE\t\tVARCHAR2(255)\tPayment type\r\n",
            "ORIGINCITY\t\t\tVARCHAR2(255)\tPick up location city\r\n",
            "ORIGINSTATE\t\tVARCHAR2(255)\tPick up location state\r\n",
            "ORIGINZIP\t\t\tVARCHAR2(255)\tPick up location zip\r\n",
            "DESTINATIONCITY\t\tVARCHAR2(255)\tDrop off location city\r\n",
            "DESTINATIONSTATE\t\tVARCHAR2(255)\tDrop off location state\r\n",
            "DESTINATIONZIP\t\tVARCHAR2(255)\tDrop off location zip\r\n",
            "MILEAGE\t\t\tVARCHAR2(255)\tTrip milaege\r\n",
            "DURATION\t\t\tVARCHAR2(255)\tTrip time - duration of \t\t\t\t\t\t\t\ttravel\r\n",
            "ORIGIN_BLOCK_LATITUDE\tNUMBER\t\tPick up location latitude\r\n",
            "ORIGIN_BLOCK_LONGITUDE/NUMBER\t\tPick up location longitude\r\n",
            "ORIGIN_BLOCKNAME\t\tVARCHAR2(255)\tPick up location street \t\t\t\t\t\t\t\tblock name\r\n",
            "DESTINATION_BLOCK_LAT\tNUMBER\t\tDrop off location latitude\r\n",
            "DESTINATION_BLOCK_LONGNUMBER\t\tDrop off location longitude\r\n",
            "DESTINATION_BLOCKNAME\tVARCHAR2(255)\tDrop off location street \t\t\t\t\t\t\t\tblock name\r\n",
            "AIRPORT\t\t\tCHAR(1)\t\tPick up or drop off \t\t\t\t\t\t\t\t\tlocation is a local airport \t\t\t\t\t\t\t(Y/N)\r\n",
            "ORIGINDATETIME_TR\tDATE\t\t\tPick up date and time\t\r\n",
            "DESTINATIONDATETIME_TRDATE\t\t\tDrop off date and time\t\r\n",
            "\r\n",
            "\r\n",
            "Payment Type\r\n",
            "1.\tCredit\r\n",
            "2.\tCash\r\n",
            "3.\tEHail \r\n",
            "4.\tOther (not sure how common this is)\r\n",
            "5.\tUber (not sure how common this is)\r\n",
            "Trip Type\r\n",
            "1.\tOrdinal (normal rate)\r\n",
            "2.\tVoD \r\n",
            "3.\tTransportDC (grant program)\r\n",
            "4.\tTransportDCShared (grant program)\r\n",
            "5.\tMOVA (grant program)\r\n",
            "6.\tCFSA (grant program)\r\n",
            "7.\tNRS (grant program)\r\n",
            "8.\tNEMT (grant program\r\n",
            "\r\n",
            "\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previewing the dataset\n",
        "\n",
        "We run the next cell to confirm that the dataset consists of pipe (|) separated values organized according to the schema described by the documentation. The `taxi_2015_09.txt` file used in the next cell was picked arbitrarily, just to illustrate the dataset."
      ],
      "metadata": {
        "id": "3SuZia-kJw-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head dctaxi/taxi_2015_09.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTNSGZHNKN1w",
        "outputId": "d945b69d-d3bd-4d8a-eda7-7a1c72806ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OBJECTID|TRIPTYPE|PROVIDERNAME|FAREAMOUNT|GRATUITYAMOUNT|SURCHARGEAMOUNT|EXTRAFAREAMOUNT|TOLLAMOUNT|TOTALAMOUNT|PAYMENTTYPE|ORIGINCITY|ORIGINSTATE|ORIGINZIP|DESTINATIONCITY|DESTINATIONSTATE|DESTINATIONZIP|MILEAGE|DURATION|ORIGIN_BLOCK_LATITUDE|ORIGIN_BLOCK_LONGITUDE|ORIGIN_BLOCKNAME|DESTINATION_BLOCK_LATITUDE|DESTINATION_BLOCK_LONGITUDE|DESTINATION_BLOCKNAME|AIRPORT|ORIGINDATETIME_TR|DESTINATIONDATETIME_TR\r\n",
            "6482604|1|Transco|3.25|0.00|0.25|0.00|0.00|3.50|1|Washington|DC|20002|Washington|DC|20002|0|1|38.897204|-77.008388|1 - 99 BLOCK OF MASSACHUSETTS AVENUE NE|38.896039|-77.007142|40 - 49 BLOCK OF COLUMBUS CIRCLE NE|N|09/01/2015 00:00|09/01/2015 00:00\r\n",
            "6482605|1|Transco|7.30|1.89|0.25|0.00|0.00|9.44|1|Washington|DC|20005|Washington|DC|20009|2|7|38.904692|-77.034573|1100 - 1199 BLOCK OF 15TH STREET NW|38.921623|-77.042365|2400 - 2499 BLOCK OF 18TH STREET NW|N|09/01/2015 00:00|09/01/2015 00:00\r\n",
            "6482606|1|Transco|5.68|0.00|0.25|0.00|0.00|5.93|2||NULL|00000||NULL|00000|1|3|||||||N|09/01/2015 00:00|09/01/2015 00:00\r\n",
            "6499791|1|Hitch|5.68|0.00|0.00|1.25|0.00|6.93|2|Washington|DC|20005|Washington|DC|20006|1|5|38.896874|-77.033647|500 - 599 BLOCK OF 15TH STREET NW|38.901336|-77.040573|1700 - 1799 BLOCK OF I STREET NW|N|09/01/2015 00:00|09/01/2015 00:00\r\n",
            "6510669|1|Hitch|19.75|0.00|0.25|0.00|0.00|19.75|1|Washington|DC|20001|Arlington|VA|22202|5|15|38.895455|-77.011189|400 - 499 BLOCK OF NEW JERSEY AVENUE NW||||Y|09/30/2015 08:00|09/30/2015 08:00\r\n",
            "6510670|1|Hitch|22.34|0.00|0.25|0.00|0.00|22.34|1|Washington|DC|00000|Arlington|VA|22202|6|21|38.904238|-77.040299|1100 - 1129 BLOCK OF CONNECTICUT AVENUE NW||||Y|09/30/2015 08:00|09/30/2015 08:00\r\n",
            "6510671|1|Hitch|12.10|0.00|0.25|0.00|0.00|12.10|1|Washington|DC|20004|Washington|DC|00000|3|11|38.89632|-77.032789|1400 - 1499 BLOCK OF PENNSYLVANIA AVENUE NW|38.885552|-77.005893|300 - 399 BLOCK OF 1ST STREET SE|N|09/30/2015 08:00|09/30/2015 08:00\r\n",
            "6510672|1|Hitch|13.60|0.00|0.25|0.00|0.00|13.60|1|Washington|DC|20036|Washington|DC|20003|3|13|38.906703|-77.038076|1615 - 1699 BLOCK OF RHODE ISLAND AVENUE NW|38.887585|-77.00861|1 - 15 BLOCK OF INDEPENDENCE AVENUE SE|N|09/30/2015 08:00|09/30/2015 08:00\r\n",
            "6510673|1|Hitch|15.51|0.00|0.25|0.00|0.00|15.51|1|Washington|DC|20008|Washington|DC|20005|3|19|38.923379|-77.054339|2400 - 2798 BLOCK OF CALVERT STREET NW|38.899817|-77.026514|1000 - 1099 BLOCK OF H STREET NW|N|09/30/2015 08:00|09/30/2015 08:00\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and install AWS CLI\n",
        "\n"
      ],
      "metadata": {
        "id": "efA4f7cgKOZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
        "unzip -o awscliv2.zip\n",
        "sudo ./aws/install"
      ],
      "metadata": {
        "id": "qqsh8WuJKVab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuring AWS credentials and region\n",
        "The contents of the next cell should be modified to specify AWS credentials as strings.\n",
        "\n",
        "**This will soon be replaced with a more secure and automated way of handling secrets*\n",
        "\n",
        "If you see the following exception:\n",
        "\n",
        "`TypeError: str expected, not NoneType`\n",
        "\n",
        "It means that you did not specify the credentials correctly."
      ],
      "metadata": {
        "id": "e4y1WyL0LOgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# *** REPLACE None in the next 2 lines with your AWS key values ***\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = None\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = None"
      ],
      "metadata": {
        "id": "z8kuFWntL9Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the next cell to validate your credentials."
      ],
      "metadata": {
        "id": "QsTEwwHxMBlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "aws sts get-caller-identity"
      ],
      "metadata": {
        "id": "XxVXMbcgMEAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# *** REPLACE None in the next line with your AWS region ***\n",
        "os.environ['AWS_DEFAULT_REGION'] = None"
      ],
      "metadata": {
        "id": "l_WUzE30MdfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have specified the region correctly, the following cell should return back the region that you have specifies."
      ],
      "metadata": {
        "id": "o_aDg6cLMjFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo $AWS_DEFAULT_REGION"
      ],
      "metadata": {
        "id": "3U98cdU2MlK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating unique bucket ID\n",
        "\n",
        "We will use the bash `$RANDOM` pseudo-random number generator and the first 32 characters of the `md5sum` output to produce a unique bucket ID."
      ],
      "metadata": {
        "id": "X2-6iN0XMSNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_ID = !echo (echo $RANDOM | md5sum | cut -c -32)\n",
        "os.environ['BUCKET_ID'] = next(iter(BUCKET_ID))\n",
        "os.environ['BUCKET_ID']"
      ],
      "metadata": {
        "id": "ckMx1kygYjsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell saves the contents of the `BUCKET_ID` environment variable to a `BUCKET_ID` file as a backup.\n",
        "\n"
      ],
      "metadata": {
        "id": "nZ2jqAbbYslI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val = os.environ['BUCKET_ID']\n",
        "%store val > BUCKET_ID\n",
        "!cat BUCKET_ID"
      ],
      "metadata": {
        "id": "i2EVPeqnYzt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the BUCKET_ID file\n",
        "\n",
        "Ensure that you have a backup copy of the `BUCKET_ID` file created by the previous cell before proceeding. The contents of the `BUCKET_ID` file are going to be reused later in this notebook and in the other notebooks."
      ],
      "metadata": {
        "id": "XRp_hKVQY6EY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creatubg an AWS bucket\n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "FiR68Q9NZIdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "aws s3api create-bucket --bucket dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION \\\n",
        "--create-bucket-configuration LocationConstraint=$AWS_DEFAULT_REGION"
      ],
      "metadata": {
        "id": "JvPi6XNSZmE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can return back the name of the bucket by running the following cell:"
      ],
      "metadata": {
        "id": "Pa-I2dz-Zubv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION"
      ],
      "metadata": {
        "id": "BM07M9yYZzL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use the AWS CLI list-buckets command to print out all the buckets that exist in your AWS account, however the printed names will not show the `s3://` prefix:"
      ],
      "metadata": {
        "id": "X8zdGR8TZ8CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3api list-buckets"
      ],
      "metadata": {
        "id": "Sj5JxOhzaASm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload the DC taxi dataset to AWS S3\n",
        "\n",
        "Synchronize the contents of the `dctaxi` directory (where you unzipped the dataset) to the `csv` folder in the S3 bucket you just created."
      ],
      "metadata": {
        "id": "XHjOlJI2aA5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "aws s3 sync \\\n",
        "  --exclude 'README*' \\\n",
        "  dctaxi/ s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/csv/"
      ],
      "metadata": {
        "id": "YgdmLRkgaOXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check whether the aws sync command completed successfully, by listing the contents of the newly created bucket."
      ],
      "metadata": {
        "id": "bcztTvHealjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 ls --recursive --summarize --human-readable s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/csv/ | tail -n 2\n"
      ],
      "metadata": {
        "id": "trD1gikZarhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating AWS role and policy to allow Glue to access the S3 bucket"
      ],
      "metadata": {
        "id": "jcp6m1AebAYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "aws iam detach-role-policy --role-name AWSGlueServiceRole-dc-taxi --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole && \\\n",
        "aws iam delete-role-policy --role-name AWSGlueServiceRole-dc-taxi --policy-name GlueBucketPolicy && \\\n",
        "aws iam delete-role --role-name AWSGlueServiceRole-dc-taxi\n",
        "\n",
        "aws iam create-role --path \"/service-role/\" --role-name AWSGlueServiceRole-dc-taxi --assume-role-policy-document '{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Principal\": {\n",
        "        \"Service\": \"glue.amazonaws.com\"\n",
        "      },\n",
        "      \"Action\": \"sts:AssumeRole\"\n",
        "    }\n",
        "  ]\n",
        "}'\n",
        "\n",
        "aws iam attach-role-policy --role-name AWSGlueServiceRole-dc-taxi --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\n",
        "\n",
        "aws iam put-role-policy --role-name AWSGlueServiceRole-dc-taxi --policy-name GlueBucketPolicy --policy-document '{\n",
        "    \"Version\": \"2012-10-17\",\n",
        "    \"Statement\": [\n",
        "        {\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"s3:*\"\n",
        "            ],\n",
        "            \"Resource\": [\n",
        "                \"arn:aws:s3:::dc-taxi-'$BUCKET_ID'-'$AWS_DEFAULT_REGION'/*\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}'"
      ],
      "metadata": {
        "id": "Zgl-GbfjbFVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an AWS Glue database"
      ],
      "metadata": {
        "id": "QeQF136ibNi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the **Glue metadata database** named `dc_taxi_db`, which is going to be used to store a schema for the\n",
        "DC taxi data set along with a table based on the schema."
      ],
      "metadata": {
        "id": "HFIljx0Toryh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "aws glue delete-database --name dc_taxi_db 2> /dev/null\n",
        "\n",
        "aws glue create-database --database-input '{\n",
        "  \"Name\": \"dc_taxi_db\"\n",
        "}'\n",
        "\n",
        "aws glue get-database --name 'dc_taxi_db'"
      ],
      "metadata": {
        "id": "ZdaQ1r0WbRHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an AWS Glue crawler\n",
        "\n",
        "Glue is as an umbrella name for a toolkit of different AWS services that you can use to\n",
        "prepare your data set for analysis.\n",
        "\n",
        "We will save the results of crawling the S3 bucket with the DC taxi dataset to the AWS Glue database created in the previous cell."
      ],
      "metadata": {
        "id": "_hVBG_24c1UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "aws glue delete-crawler --name dc-taxi-csv-crawler 2> /dev/null\n",
        "\n",
        "aws glue create-crawler \\\n",
        "  --name dc-taxi-csv-crawler \\\n",
        "  --database-name dc_taxi_db \\\n",
        "  --table-prefix dc_taxi_ \\\n",
        "  --role $( aws iam get-role \\\n",
        "              --role-name AWSGlueServiceRole-dc-taxi \\\n",
        "              --query 'Role.Arn' \\\n",
        "              --output text ) \\\n",
        "   --targets '{\n",
        "  \"S3Targets\": [\n",
        "    {\n",
        "      \"Path\": \"s3://dc-taxi-'$BUCKET_ID'-'$AWS_DEFAULT_REGION'/csv\"\n",
        "    }]\n",
        "}'\n",
        "\n",
        "aws glue start-crawler --name dc-taxi-csv-crawler"
      ],
      "metadata": {
        "id": "pqgRn-3udAMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6YnkCfdion8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the status of the AWS Glue crawler\n",
        "\n"
      ],
      "metadata": {
        "id": "ZjsVRrw2dDGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "aws glue get-crawler --name dc-taxi-csv-crawler --query 'Crawler.State' --output text"
      ],
      "metadata": {
        "id": "miLNAUrcdK7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poll the crawler state every minute to wait for it to finish."
      ],
      "metadata": {
        "id": "2EAm3ALTdMji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "printf \"Waiting for crawler to finish...\"\n",
        "until echo \"$(aws glue get-crawler --name dc-taxi-csv-crawler --query 'Crawler.State' --output text)\" | grep -q \"READY\"; do\n",
        "   sleep 60\n",
        "   printf \"...\"\n",
        "done\n",
        "printf \"done\\n\""
      ],
      "metadata": {
        "id": "cCZGU0XmdPha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding out the last known status of the AWS Glue crawler\n",
        "\n"
      ],
      "metadata": {
        "id": "28DQyYkVdWWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws glue get-crawler --name dc-taxi-csv-crawler --query 'Crawler.LastCrawl'"
      ],
      "metadata": {
        "id": "wy_-MTFNdZSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Describe the table created by the AWS Glue crawler\n",
        "\n"
      ],
      "metadata": {
        "id": "nb1TX8BTdeR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws glue get-table --database-name dc_taxi_db --name dc_taxi_csv"
      ],
      "metadata": {
        "id": "B3IJXf5cdhdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a PySpark job to convert CSV to Parquet\n",
        "\n"
      ],
      "metadata": {
        "id": "LYZSAqtqdjZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dctaxi_csv_to_parquet.py\n",
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME',\n",
        "                                     'BUCKET_SRC_PATH',\n",
        "                                     'BUCKET_DST_PATH',\n",
        "\t\t\t\t\t\t\t\t\t                    'DST_VIEW_NAME'])\n",
        "\n",
        "BUCKET_SRC_PATH = args['BUCKET_SRC_PATH']\n",
        "BUCKET_DST_PATH = args['BUCKET_DST_PATH']\n",
        "DST_VIEW_NAME = args['DST_VIEW_NAME']\n",
        "\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "logger = glueContext.get_logger()\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "df = ( spark.read.format(\"csv\")\n",
        "\t\t.option(\"header\", True)\n",
        "\t\t.option(\"inferSchema\", True)\n",
        "    .option(\"multiLine\", True)\n",
        "\t\t.option(\"delimiter\", \"|\")\n",
        "\t\t.load(f\"{BUCKET_SRC_PATH}\") )\n",
        "\n",
        "df.createOrReplaceTempView(f\"{DST_VIEW_NAME}\")\n",
        "\n",
        "query_df = spark.sql(f\"\"\"\n",
        "SELECT\n",
        "\n",
        "  CAST(fareamount AS DOUBLE) AS fareamount_double,\n",
        "  CAST(fareamount AS STRING) AS fareamount_string,\n",
        "\n",
        "  CAST(origindatetime_tr AS STRING) AS origindatetime_tr,\n",
        "\n",
        "  CAST(origin_block_latitude AS DOUBLE) AS origin_block_latitude_double,\n",
        "  CAST(origin_block_latitude AS STRING) AS origin_block_latitude_string,\n",
        "\n",
        "  CAST(origin_block_longitude AS DOUBLE) AS origin_block_longitude_double,\n",
        "  CAST(origin_block_longitude AS STRING) AS origin_block_longitude_string,\n",
        "\n",
        "  CAST(destination_block_latitude AS DOUBLE) AS destination_block_latitude_double,\n",
        "  CAST(destination_block_latitude AS STRING) AS destination_block_latitude_string,\n",
        "\n",
        "  CAST(destination_block_longitude AS DOUBLE) AS destination_block_longitude_double,\n",
        "  CAST(destination_block_longitude AS STRING) AS destination_block_longitude_string,\n",
        "\n",
        "  CAST(mileage AS DOUBLE) AS mileage_double,\n",
        "  CAST(mileage AS STRING) AS mileage_string\n",
        "\n",
        "FROM {DST_VIEW_NAME}\"\"\".replace('\\n', ''))\n",
        "\n",
        "query_df.write.parquet(f\"{BUCKET_DST_PATH}\", mode=\"overwrite\")\n",
        "\n",
        "job.commit()"
      ],
      "metadata": {
        "id": "Rf55MCZwdpcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copying the PySpark job code to the S3 bucket"
      ],
      "metadata": {
        "id": "rhAU_mfSdsd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "aws s3 cp dctaxi_csv_to_parquet.py s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/glue/\n",
        "aws s3 ls s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/glue/dctaxi_csv_to_parquet.py"
      ],
      "metadata": {
        "id": "gToAi0wodvUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating and starting the PySpark job"
      ],
      "metadata": {
        "id": "_EbLy4elusWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "aws glue delete-job --job-name dc-taxi-csv-to-parquet-job 2> /dev/null\n",
        "\n",
        "aws glue create-job \\\n",
        "  --name dc-taxi-csv-to-parquet-job \\\n",
        "  --role $(aws iam get-role --role-name AWSGlueServiceRole-dc-taxi --query 'Role.Arn' --output text) \\\n",
        "  --default-arguments '{\"--TempDir\":\"s3://dc-taxi-'$BUCKET_ID'-'$AWS_DEFAULT_REGION'/glue/\"}' \\\n",
        "  --command '{\n",
        "    \"ScriptLocation\": \"s3://dc-taxi-'$BUCKET_ID'-'$AWS_DEFAULT_REGION'/glue/dctaxi_csv_to_parquet.py\",\n",
        "    \"Name\": \"glueetl\",\n",
        "    \"PythonVersion\": \"3\"\n",
        "  }' \\\n",
        "  --glue-version \"2.0\"\n",
        "\n",
        "aws glue start-job-run \\\n",
        "  --job-name dc-taxi-csv-to-parquet-job \\\n",
        "  --arguments='--BUCKET_SRC_PATH=\"'$(\n",
        "      echo s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/csv/\n",
        "    )'\",\n",
        "  --BUCKET_DST_PATH=\"'$(\n",
        "      echo s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/parquet/\n",
        "    )'\",\n",
        "  --DST_VIEW_NAME=\"dc_taxi_csv\"'"
      ],
      "metadata": {
        "id": "VqXna-S8uwN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the PySpark job completes successfully, the job execution status should change from `RUNNING` to `SUCCEEDED`. We can re-run the next cell to get the updated job status."
      ],
      "metadata": {
        "id": "AQZnOT922-rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws glue get-job-runs --job-name dc-taxi-csv-to-parquet-job --output text --query 'JobRuns[0].JobRunState'"
      ],
      "metadata": {
        "id": "r4J-_llk3D4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poll the job every minute to wait for it to finish"
      ],
      "metadata": {
        "id": "soeGYglz3FX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "printf \"Waiting for the job to finish...\"\n",
        "while echo \"$(aws glue get-job-runs --job-name dc-taxi-csv-to-parquet-job --query 'JobRuns[0].JobRunState')\" | grep -q -E \"STARTING|RUNNING|STOPPING\"; do\n",
        "   sleep 60\n",
        "   printf \"...\"\n",
        "done\n",
        "aws glue get-job-runs --job-name dc-taxi-csv-to-parquet-job --output text --query 'JobRuns[0].JobRunState'\n",
        ""
      ],
      "metadata": {
        "id": "ew9WVPEA3IAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the job succeeds, you can list the contents of the parquet folder in the bucket\n",
        "using"
      ],
      "metadata": {
        "id": "9rxe33M23USy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 ls --recursive --summarize --human-readable s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/parquet/"
      ],
      "metadata": {
        "id": "5vuVBdXu3Wd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crawling the Parquet dataset"
      ],
      "metadata": {
        "id": "PlF2_iZO3rq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "aws glue delete-crawler --name dc-taxi-parquet-crawler  2> /dev/null\n",
        "\n",
        "aws glue create-crawler --name dc-taxi-parquet-crawler --database-name dc_taxi_db --table-prefix dc_taxi_ --role `aws iam get-role --role-name AWSGlueServiceRole-dc-taxi --query 'Role.Arn' --output text` --targets '{\n",
        "  \"S3Targets\": [\n",
        "    {\n",
        "      \"Path\": \"s3://dc-taxi-'$BUCKET_ID'-'$AWS_DEFAULT_REGION'/parquet/\"\n",
        "    }]\n",
        "}'\n",
        "\n",
        "aws glue start-crawler --name dc-taxi-parquet-crawler"
      ],
      "metadata": {
        "id": "dLOQ9vpv3vCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitoring the Parquet crawler status\n",
        "\n",
        "Poll the crawler status every minute to wait for it to finish"
      ],
      "metadata": {
        "id": "o9ZvwGgS3ySV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "printf \"Waiting for crawler to finish...\"\n",
        "until echo \"$(aws glue get-crawler --name dc-taxi-parquet-crawler --query 'Crawler.State' --output text)\" | grep -q \"READY\"; do\n",
        "   sleep 10\n",
        "   printf \"...\"\n",
        "done\n",
        "aws glue get-crawler --name dc-taxi-parquet-crawler --query 'Crawler.State' --output text"
      ],
      "metadata": {
        "id": "jOrgzjkA36fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confirming that the crawler successfully created the `dc_taxi_parquet` table\n",
        "\n",
        "If the crawler completed successfully, the number of records in the `dc_taxi_parquet` table should be equal to `53173692`"
      ],
      "metadata": {
        "id": "Qt_Wduc33892"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws glue get-table --database-name dc_taxi_db --name dc_taxi_parquet --query \"Table.Parameters.recordCount\" --output text\n"
      ],
      "metadata": {
        "id": "OGPaqbCf4MnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}